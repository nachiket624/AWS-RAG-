ğŸ“˜ Retrieval-Augmented Generation (RAG) Pipeline on AWS

This project implements a complete Retrieval-Augmented Generation (RAG) workflow using Amazon Bedrock, AWS Glue, Amazon OpenSearch Service, Amazon DynamoDB, and Amazon S3.
The goal is to build a scalable, secure, and production-ready architecture that extracts knowledge from documents and allows LLMs to answer questions using that knowledge.

ğŸ—ï¸ Architecture Overview

ğŸ”¹ Components
1. Amazon S3 (Document Storage)

All input documentsâ€”PDFs, text files, Word files, reportsâ€”are stored in an S3 bucket.
This acts as the central storage for the entire knowledge ingestion pipeline.

2. AWS Glue (ETL Jobs)

AWS Glue jobs perform:

Document extraction

Text parsing

Chunking/splitting

Preprocessing (cleaning, formatting)

The output is ready for vector embedding generation.

3. Amazon Bedrock (Embeddings)

Processed chunks from Glue are passed to Amazon Bedrockâ€™s embedding model.
Bedrock transforms each document chunk into a numerical vector representation.

4. Vector Store

Vector embeddings and metadata are stored in:

Amazon OpenSearch Service â€” for vector similarity search

Amazon DynamoDB â€” for metadata indexing and fast key-value lookups

This forms the knowledge base that powers retrieval.

5. Amazon Bedrock (LLM â€“ Llama3)

A Bedrock-hosted LLM generates accurate responses based on:

User query

Retrieved context from the vector store

This enables high-quality Retrieval-Augmented Generation.

ğŸš€ End-to-End Workflow

Upload documents to Amazon S3.

AWS Glue extracts and preprocesses the text.

Clean chunks are sent to Amazon Bedrock Embeddings.

Generated vectors are stored in OpenSearch alongside metadata in DynamoDB.

User submits a query.

System retrieves semantically similar document chunks from OpenSearch.

Retrieved context is fed to Bedrock LLM (Llama3).

The LLM generates a final grounded answer.

ğŸ§© Features

ğŸ”¹ End-to-end serverless ingestion

ğŸ”¹ Highly scalable vector search engine

ğŸ”¹ Fast and reliable metadata indexing

ğŸ”¹ Bedrock-powered embeddings and LLM reasoning

ğŸ”¹ Suitable for enterprise knowledge bases and AI search applications

ğŸ“¦ Tech Stack
Layer	Service
Storage	Amazon S3
ETL	AWS Glue
Embeddings	Amazon Bedrock
Vector Search	Amazon OpenSearch Service
Metadata DB	DynamoDB
LLM	Amazon Bedrock (Llama3 or other models)
